{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#lists needed for data storage\n",
    "Influncer_username = []\n",
    "Influncer_bio = []\n",
    "\n",
    "#website_to_be_scraped\n",
    "website1 = requests.get(\"https://influencers.feedspot.com/egyptian_instagram_influencers/\")\n",
    "\n",
    "#source_contente\n",
    "source = website1.content\n",
    "\n",
    "#souping_url\n",
    "soup = BeautifulSoup(source, \"lxml\")\n",
    "\n",
    "#Influncers bio and user names for top 97 influncers in egypt instagram\n",
    "Influncer_namedata = soup.find_all('a', class_='ins_dhl')\n",
    "Influncer_biodata = soup.find_all('p', class_='trow trow-wrap')\n",
    "# Check if the list contains 97 values this means the list is correct\n",
    "print(len(Influncer_namedata))\n",
    "print(len(Influncer_biodata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 97\n"
     ]
    }
   ],
   "source": [
    "#Loop through the lists to extract the desired user name and bio\n",
    "for name in Influncer_namedata:\n",
    "    username = name.text\n",
    "    Influncer_username.append(username)\n",
    "for bio in Influncer_biodata:\n",
    "    bio = bio.text\n",
    "    Influncer_bio.append(bio)\n",
    "\n",
    "Influncer_username_stripped = [username.strip() for username in Influncer_username]\n",
    "# Check if the list contains 97 values this means the list is correct\n",
    "print(len(Influncer_username_stripped),len(Influncer_bio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the web driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the URL\n",
    "url = 'https://inflact.com/tools/profile-analyzer/'\n",
    "driver.get(url)\n",
    "\n",
    "# Define lists to store the extracted data\n",
    "post_per_day = []\n",
    "posts_per_week = []\n",
    "post_per_month = []\n",
    "total_posts = []\n",
    "Engagement_rate = []\n",
    "total_followers = []\n",
    "Avg_user_activity = []\n",
    "\n",
    "# Loop through each influencer username\n",
    "for username in Influncer_username_stripped:\n",
    "    # Wait until the input field for username is present\n",
    "    username_input = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"profileanalyzerform-username\"]')))\n",
    "\n",
    "    # Enter the username in the input field\n",
    "    username_input.send_keys(username)\n",
    "\n",
    "    # Wait until the search button is present and click it\n",
    "    submit_button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"account-form\"]/div[2]/div[2]/button')))\n",
    "    submit_button.click()\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(30)\n",
    "\n",
    "    # Get the HTML source code of the page a queastion will rise here why we do this and we are not using beautifulsoap directly ?\n",
    "    #for this website I need to pass the username to get search results so we use selenum to do this then scrape the data using beautiful soap for shorter codr running time\n",
    "    html_source = driver.page_source\n",
    "\n",
    "    # Pass the HTML source to Beautiful Soup for parsing\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "    # Extract the desired metrics using Beautiful Soup\n",
    "    #for the bar charts we have there numbers we need so we will get a list of all desired numbers in bar charts then extract the data from it\n",
    "    #create a list to store the data and scrape the bar charts\n",
    "    # Bar elements\n",
    "    bar_chart_data = soup.find_all(\"div\", class_=\"pa-chart-data-number\")\n",
    "\n",
    "\n",
    "    #note: we are using try except becouse sometimes the list is empty due to a private account or whatever and we don't want our code to crash\n",
    "    try:\n",
    "        number_of_posts_per_day = bar_chart_data[0].get_text(strip=True)\n",
    "        post_per_day.append(number_of_posts_per_day)\n",
    "    except IndexError:\n",
    "        post_per_day.append(\"\")\n",
    "\n",
    "    try:\n",
    "        number_of_posts_per_week = bar_chart_data[1].get_text(strip=True)\n",
    "        posts_per_week.append(number_of_posts_per_week)\n",
    "    except IndexError:\n",
    "        posts_per_week.append(\"\")\n",
    "\n",
    "    try:\n",
    "        number_of_posts_per_month = bar_chart_data[2].get_text(strip=True)\n",
    "        post_per_month.append(number_of_posts_per_month)\n",
    "    except IndexError:\n",
    "        post_per_month.append(\"\")\n",
    "\n",
    "\n",
    "\n",
    "    # Extract the desired metrics using Beautiful Soup\n",
    "    #for the boxes we have four numbers we need so we will get a list of all desired numbers in boxes then extract the data from it\n",
    "    #create a list to store the data and scrape the bar charts\n",
    "    bar_chart_data =[]\n",
    "\n",
    "    box_values_data = soup.find_all(\"div\", class_=\"pa-number-value\")\n",
    "    # Box elements\n",
    "    box_elements = soup.find_all(\"div\", class_=\"pa-number-value\")\n",
    "\n",
    "    #note: we are using try except becouse sometimes the list is empty due to a private account or whatever and we don't want our code to crash\n",
    "    try:\n",
    "        number_of_post = box_values_data[0].get_text(strip=True)\n",
    "        total_posts.append(number_of_post)\n",
    "    except IndexError:\n",
    "        total_posts.append(\"\")\n",
    "\n",
    "    try:\n",
    "        Engagmentrate = box_values_data[1].get_text(strip=True)\n",
    "        Engagement_rate.append(Engagmentrate)\n",
    "    except IndexError:\n",
    "        Engagement_rate.append(\"\")\n",
    "\n",
    "    try:\n",
    "        numberof_followers = box_values_data[2].get_text(strip=True)\n",
    "        total_followers.append(numberof_followers)\n",
    "    except IndexError:\n",
    "        total_followers.append(\"\")\n",
    "\n",
    "    try:\n",
    "        avg_activity = box_values_data[3].get_text(strip=True)\n",
    "        Avg_user_activity.append(avg_activity)\n",
    "    except IndexError:\n",
    "        Avg_user_activity.append(\"\")\n",
    "\n",
    "    # Re-locate the input field for the next iteration\n",
    "    username_input = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"profileanalyzerform-username\"]')))\n",
    "\n",
    "    # Clear the input field for the next username\n",
    "    username_input.clear()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "# Create a pandas DataFrame to store the extracted data\n",
    "data = pd.DataFrame({\n",
    "    'Username': Influncer_username_stripped,\n",
    "    'Posts per Day': post_per_day,\n",
    "    'Posts per Week': posts_per_week,\n",
    "    'Posts per Month': post_per_month,\n",
    "    'Total Posts': total_posts,\n",
    "    'Engagement Rate': Engagement_rate,\n",
    "    'Total Followers': total_followers,\n",
    "    'Average User Activity': Avg_user_activity\n",
    "})\n",
    "\n",
    "# Print or save the data as desired\n",
    "# Close the web driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame to store the extracted data\n",
    "data = pd.DataFrame({\n",
    "    'Username': Influncer_username_stripped,\n",
    "    'Posts per Day': post_per_day,\n",
    "    'Posts per Week': posts_per_week,\n",
    "    'Posts per Month': post_per_month,\n",
    "    'Total Posts': total_posts,\n",
    "    'Engagement Rate': Engagement_rate,\n",
    "    'Total Followers': total_followers,\n",
    "    'Average User Activity': Avg_user_activity,\n",
    "    'Influncer_bio': Influncer_bio\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the data to a csv file\n",
    "data.to_csv('instagram top 100 influncer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
